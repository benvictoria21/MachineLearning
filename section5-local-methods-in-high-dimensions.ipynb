{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"section5-local-methods-in-high-dimensions.ipynb","provenance":[],"private_outputs":true,"authorship_tag":"ABX9TyPRM6G3N3g63OWHi9wgEPQz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZrRtyZNDgRU-","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import math\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vNdCUogSggMs","colab_type":"code","colab":{}},"source":["\"\"\"FIGURE 2.6. (right panel) The unit hypercube example\"\"\"\n","fraction_of_volume = np.arange(0, 1, 0.001)\n","edge_length_p1 = fraction_of_volume\n","edge_length_p2 = fraction_of_volume**.5\n","edge_length_p3 = fraction_of_volume**(1/3)\n","edge_length_p10 = fraction_of_volume**.1\n","\n","fig1 = plt.figure(1)\n","ax11 = fig1.add_subplot(1, 1, 1)\n","ax11.plot(fraction_of_volume, edge_length_p10, label='p=10')\n","ax11.plot(fraction_of_volume, edge_length_p3, label='p=3')\n","ax11.plot(fraction_of_volume, edge_length_p2, label='p=2')\n","ax11.plot(fraction_of_volume, edge_length_p1, label='p=1')\n","ax11.set_xlabel('Fraction of Volume')\n","ax11.set_ylabel('Distance')\n","ax11.legend()\n","ax11.plot([.1, .1], [0, 1], '--', color='C0', alpha=.5)\n","ax11.plot([.3, .3], [0, 1], '--', color='C0', alpha=.5)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2PAEl2Lgieh","colab_type":"code","colab":{}},"source":["\"\"\"FIGURE 2.7. (bottom panels) Bias-variance decomposition example.\n","Given the dimension p, 100 simulations are done and the following steps are\n","taken for each simulation.\n","1. Generate data of size 1000 from [-1, 1]^p\n","2. Grap the nearest neighbor x of 0 and calculate the distance, i.e., norm\n","3. Calculate y=f(x) and the variance and the squared bias for simulation\n","of size 100.\n","\"\"\"\n","def generate_data(p: int, n: int) ->np.ndarray:\n","    if p == 1:\n","        return np.array([random.uniform(-1, 1) for _ in range(n)])\n","    return np.array([\n","        [random.uniform(-1, 1) for _ in range(p)]\n","        for _ in range(n)\n","    ])\n","\n","\n","def f(p: int, x: np.ndarray) ->float:\n","    if p == 1:\n","        return math.exp(-8*(x**2))\n","    return math.exp(-8*sum(xi*xi for xi in x))\n","\n","\n","def simulate(p: int, nsample:int, nsim: int) ->dict:\n","    res = {'average_distance': 0}\n","    sum_y = 0\n","    sum_y_square = 0\n","    for _ in range(nsim):\n","        data = generate_data(p, nsample)\n","        if p == 1:\n","            data_norm = np.abs(data)\n","        else:\n","            data_norm = np.linalg.norm(data, ord=2, axis=1)\n","        nearest_index = data_norm.argmin()\n","        nearest_x, nearest_distance = data[nearest_index], data_norm[nearest_index]\n","        nearest_y = f(p, nearest_x)\n","        sum_y += nearest_y\n","        sum_y_square += nearest_y*nearest_y\n","        res['average_distance'] += nearest_distance\n","    average_y = sum_y/nsim\n","    res['average_distance'] /= nsim\n","    res['variance'] = sum_y_square/nsim - average_y*average_y\n","    res['squared_bias'] = (1-average_y)*(1-average_y)\n","    return res\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eDr6NCixgr_L","colab_type":"code","colab":{}},"source":["nsim = 100\n","data = {p: simulate(p, 1000, nsim) for p in range(1, 11)}\n","dimension = list(data.keys())\n","average_distance = [d['average_distance'] for p, d in data.items()]\n","variance = np.array([d['variance'] for p, d in data.items()])\n","squared_bias = np.array([d['squared_bias'] for p, d in data.items()])\n","mse = variance + squared_bias\n","\n","fig2 = plt.figure(2, figsize=(10, 5))\n","ax21 = fig2.add_subplot(1, 2, 1)\n","ax21.set_title('Distance to 1-NN vs. Dimension')\n","ax21.plot(dimension, average_distance, 'ro--')\n","ax21.set_xlabel('Dimension')\n","ax21.set_ylabel('Average Distance to Nearest Neighbor')\n","\n","ax22 = fig2.add_subplot(1, 2, 2)\n","ax22.set_title('MSE vs. Dimension')\n","ax22.plot(dimension, mse, 'o-', label='MSE')\n","ax22.plot(dimension, variance, 'o-', label='Variance')\n","ax22.plot(dimension, squared_bias, 'o-', label='Squared Bias')\n","ax22.set_xlabel('Dimension')\n","ax22.set_ylabel('MSE')\n","ax22.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUmJDZHpgveH","colab_type":"code","colab":{}},"source":["\"\"\"FIGURE 2.8. The variance-dominating example.\"\"\"\n","print('Please check this later ...')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vowB9erlgyOa","colab_type":"code","colab":{}},"source":["\"\"\"FIGURE 2.8. The variance-dominating example.\"\"\"\n","print('Please check this later ...')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWMV2f5hmnI-","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}