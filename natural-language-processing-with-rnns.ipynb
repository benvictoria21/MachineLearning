{"cells":[{"metadata":{"id":"5KiCCBsIkMHi","trusted":true},"cell_type":"code","source":"vocab = {}  # maps word to integer representing it\nword_encoding = 1\ndef bag_of_words(text):\n  global word_encoding\n\n  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n  bag = {}  # stores all of the encodings and their frequency\n\n  for word in words:\n    if word in vocab:\n      encoding = vocab[word]  # get encoding from vocab\n    else:\n      vocab[word] = word_encoding\n      encoding = word_encoding\n      word_encoding += 1\n    \n    if encoding in bag:\n      bag[encoding] += 1\n    else:\n      bag[encoding] = 1\n  \n  return bag\n\ntext = \"this is a test to see if this test will work is is test a a\"\nbag = bag_of_words(text)\nprint(bag)\nprint(vocab)","execution_count":1,"outputs":[{"output_type":"stream","text":"{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n","name":"stdout"}]},{"metadata":{"id":"miYshfvzmJ0H","trusted":true},"cell_type":"code","source":"positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\nnegative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n\npos_bag = bag_of_words(positive_review)\nneg_bag = bag_of_words(negative_review)\n\nprint(\"Positive:\", pos_bag)\nprint(\"Negative:\", neg_bag)","execution_count":2,"outputs":[{"output_type":"stream","text":"Positive: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1}\nNegative: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 21: 1, 18: 1, 19: 1, 20: 1, 17: 1}\n","name":"stdout"}]},{"metadata":{"id":"MKY4y_tjnUEW","trusted":true},"cell_type":"code","source":"vocab = {}  \nword_encoding = 1\ndef one_hot_encoding(text):\n  global word_encoding\n\n  words = text.lower().split(\" \") \n  encoding = []  \n\n  for word in words:\n    if word in vocab:\n      code = vocab[word]  \n      encoding.append(code) \n    else:\n      vocab[word] = word_encoding\n      encoding.append(word_encoding)\n      word_encoding += 1\n  \n  return encoding\n\ntext = \"this is a test to see if this test will work is is test a a\"\nencoding = one_hot_encoding(text)\nprint(encoding)\nprint(vocab)","execution_count":3,"outputs":[{"output_type":"stream","text":"[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n","name":"stdout"}]},{"metadata":{"id":"TOrLG9Bin0Zv"},"cell_type":"markdown","source":"And now let's have a look at one hot encoding on our movie reviews."},{"metadata":{"id":"1S-GNjotn-Br","trusted":true},"cell_type":"code","source":"positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\nnegative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n\npos_encode = one_hot_encoding(positive_review)\nneg_encode = one_hot_encoding(negative_review)\n\nprint(\"Positive:\", pos_encode)\nprint(\"Negative:\", neg_encode)","execution_count":4,"outputs":[{"output_type":"stream","text":"Positive: [10, 11, 12, 13, 14, 15, 5, 16, 17, 18, 19, 14, 20, 21]\nNegative: [10, 11, 12, 13, 14, 15, 5, 16, 21, 18, 19, 14, 20, 17]\n","name":"stdout"}]},{"metadata":{"id":"pdsus1kyXWC8","trusted":true},"cell_type":"code","source":"from keras.datasets import imdb\nfrom keras.preprocessing import sequence\nimport keras\nimport tensorflow as tf\nimport os\nimport numpy as np\n\nVOCAB_SIZE = 88584\n\nMAXLEN = 250\nBATCH_SIZE = 64\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)","execution_count":5,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17465344/17464789 [==============================] - 1s 0us/step\n","name":"stdout"}]},{"metadata":{"id":"Wh6lOpcQ9sIZ","trusted":true},"cell_type":"code","source":"# Lets look at one review\ntrain_data[1]","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"[1,\n 194,\n 1153,\n 194,\n 8255,\n 78,\n 228,\n 5,\n 6,\n 1463,\n 4369,\n 5012,\n 134,\n 26,\n 4,\n 715,\n 8,\n 118,\n 1634,\n 14,\n 394,\n 20,\n 13,\n 119,\n 954,\n 189,\n 102,\n 5,\n 207,\n 110,\n 3103,\n 21,\n 14,\n 69,\n 188,\n 8,\n 30,\n 23,\n 7,\n 4,\n 249,\n 126,\n 93,\n 4,\n 114,\n 9,\n 2300,\n 1523,\n 5,\n 647,\n 4,\n 116,\n 9,\n 35,\n 8163,\n 4,\n 229,\n 9,\n 340,\n 1322,\n 4,\n 118,\n 9,\n 4,\n 130,\n 4901,\n 19,\n 4,\n 1002,\n 5,\n 89,\n 29,\n 952,\n 46,\n 37,\n 4,\n 455,\n 9,\n 45,\n 43,\n 38,\n 1543,\n 1905,\n 398,\n 4,\n 1649,\n 26,\n 6853,\n 5,\n 163,\n 11,\n 3215,\n 10156,\n 4,\n 1153,\n 9,\n 194,\n 775,\n 7,\n 8255,\n 11596,\n 349,\n 2637,\n 148,\n 605,\n 15358,\n 8003,\n 15,\n 123,\n 125,\n 68,\n 23141,\n 6853,\n 15,\n 349,\n 165,\n 4362,\n 98,\n 5,\n 4,\n 228,\n 9,\n 43,\n 36893,\n 1157,\n 15,\n 299,\n 120,\n 5,\n 120,\n 174,\n 11,\n 220,\n 175,\n 136,\n 50,\n 9,\n 4373,\n 228,\n 8255,\n 5,\n 25249,\n 656,\n 245,\n 2350,\n 5,\n 4,\n 9837,\n 131,\n 152,\n 491,\n 18,\n 46151,\n 32,\n 7464,\n 1212,\n 14,\n 9,\n 6,\n 371,\n 78,\n 22,\n 625,\n 64,\n 1382,\n 9,\n 8,\n 168,\n 145,\n 23,\n 4,\n 1690,\n 15,\n 16,\n 4,\n 1355,\n 5,\n 28,\n 6,\n 52,\n 154,\n 462,\n 33,\n 89,\n 78,\n 285,\n 16,\n 145,\n 95]"},"metadata":{}}]},{"metadata":{"id":"Z3qQ83sNeog6","trusted":true},"cell_type":"code","source":"train_data = sequence.pad_sequences(train_data, MAXLEN)\ntest_data = sequence.pad_sequences(test_data, MAXLEN)","execution_count":7,"outputs":[]},{"metadata":{"id":"OWGGcBIpjrMu","trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n    tf.keras.layers.LSTM(32),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])","execution_count":8,"outputs":[]},{"metadata":{"id":"O8_jPL_Kkr-a","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":9,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, None, 32)          2834688   \n_________________________________________________________________\nlstm (LSTM)                  (None, 32)                8320      \n_________________________________________________________________\ndense (Dense)                (None, 1)                 33        \n=================================================================\nTotal params: 2,843,041\nTrainable params: 2,843,041\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"id":"KKEMjaIulPBe","trusted":true},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n\nhistory = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)","execution_count":10,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n625/625 [==============================] - 16s 25ms/step - loss: 0.4169 - acc: 0.8094 - val_loss: 0.7275 - val_acc: 0.7476\nEpoch 2/10\n625/625 [==============================] - 14s 22ms/step - loss: 0.2367 - acc: 0.9092 - val_loss: 0.3057 - val_acc: 0.8826\nEpoch 3/10\n625/625 [==============================] - 13s 21ms/step - loss: 0.1835 - acc: 0.9335 - val_loss: 0.3943 - val_acc: 0.8574\nEpoch 4/10\n625/625 [==============================] - 14s 22ms/step - loss: 0.1473 - acc: 0.9477 - val_loss: 0.2976 - val_acc: 0.8900\nEpoch 5/10\n625/625 [==============================] - 14s 22ms/step - loss: 0.1246 - acc: 0.9556 - val_loss: 0.3133 - val_acc: 0.8888\nEpoch 6/10\n625/625 [==============================] - 13s 21ms/step - loss: 0.1076 - acc: 0.9636 - val_loss: 0.3393 - val_acc: 0.8816\nEpoch 7/10\n625/625 [==============================] - 13s 21ms/step - loss: 0.0912 - acc: 0.9692 - val_loss: 0.3961 - val_acc: 0.8808\nEpoch 8/10\n625/625 [==============================] - 14s 22ms/step - loss: 0.0768 - acc: 0.9744 - val_loss: 0.3086 - val_acc: 0.8874\nEpoch 9/10\n625/625 [==============================] - 13s 21ms/step - loss: 0.0640 - acc: 0.9790 - val_loss: 0.3972 - val_acc: 0.8868\nEpoch 10/10\n625/625 [==============================] - 13s 21ms/step - loss: 0.0566 - acc: 0.9820 - val_loss: 0.4729 - val_acc: 0.8686\n","name":"stdout"}]},{"metadata":{"id":"KImNMWTDoJaQ","trusted":true},"cell_type":"code","source":"results = model.evaluate(test_data, test_labels)\nprint(results)","execution_count":11,"outputs":[{"output_type":"stream","text":"782/782 [==============================] - 4s 5ms/step - loss: 0.6263 - acc: 0.8312\n[0.6263226270675659, 0.8311600089073181]\n","name":"stdout"}]},{"metadata":{"id":"Onu8leY4Cn9z","trusted":true},"cell_type":"code","source":"word_index = imdb.get_word_index()\n\ndef encode_text(text):\n  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n  return sequence.pad_sequences([tokens], MAXLEN)[0]\n\ntext = \"that movie was just amazing, so amazing\"\nencoded = encode_text(text)\nprint(encoded)\n","execution_count":12,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n1646592/1641221 [==============================] - 0s 0us/step\n[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n","name":"stdout"}]},{"metadata":{"id":"PKna3vxmFwrB","trusted":true},"cell_type":"code","source":"# while were at it lets make a decode function\n\nreverse_word_index = {value: key for (key, value) in word_index.items()}\n\ndef decode_integers(integers):\n    PAD = 0\n    text = \"\"\n    for num in integers:\n      if num != PAD:\n        text += reverse_word_index[num] + \" \"\n\n    return text[:-1]\n  \nprint(decode_integers(encoded))","execution_count":13,"outputs":[{"output_type":"stream","text":"that movie was just amazing so amazing\n","name":"stdout"}]},{"metadata":{"id":"L8nyrr00HPZF","trusted":true},"cell_type":"code","source":"# now time to make a prediction\n\ndef predict(text):\n  encoded_text = encode_text(text)\n  pred = np.zeros((1,250))\n  pred[0] = encoded_text\n  result = model.predict(pred) \n  print(result[0])\n\npositive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\npredict(positive_review)\n\nnegative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\npredict(negative_review)\n","execution_count":14,"outputs":[{"output_type":"stream","text":"[0.85011977]\n[0.28074044]\n","name":"stdout"}]},{"metadata":{"id":"fju7i1FKrK_G","trusted":true},"cell_type":"code","source":"from keras.preprocessing import sequence\nimport keras\nimport tensorflow as tf\nimport os\nimport numpy as np","execution_count":16,"outputs":[]},{"metadata":{"id":"IdRcVIhtRGlF","trusted":true},"cell_type":"code","source":"path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')","execution_count":17,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n1122304/1115394 [==============================] - 0s 0us/step\n","name":"stdout"}]},{"metadata":{"id":"-n4oovOMRnP7","trusted":true},"cell_type":"code","source":"# Read, then decode for py2 compat.\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n# length of text is the number of characters in it\nprint ('Length of text: {} characters'.format(len(text)))","execution_count":21,"outputs":[{"output_type":"stream","text":"Length of text: 1115394 characters\n","name":"stdout"}]},{"metadata":{"id":"KHUxQVl7Rt10","trusted":true},"cell_type":"code","source":"# Take a look at the first 250 characters in text\nprint(text[:250])","execution_count":22,"outputs":[{"output_type":"stream","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n","name":"stdout"}]},{"metadata":{"id":"C7AZNI7aRz6y","trusted":true},"cell_type":"code","source":"vocab = sorted(set(text))\n# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ndef text_to_int(text):\n  return np.array([char2idx[c] for c in text])\n\ntext_as_int = text_to_int(text)","execution_count":23,"outputs":[]},{"metadata":{"id":"_i5kvmX_SLW4","trusted":true},"cell_type":"code","source":"# lets look at how part of our text is encoded\nprint(\"Text:\", text[:13])\nprint(\"Encoded:\", text_to_int(text[:13]))","execution_count":24,"outputs":[{"output_type":"stream","text":"Text: First Citizen\nEncoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n","name":"stdout"}]},{"metadata":{"id":"Af52YChSW5hX","trusted":true},"cell_type":"code","source":"def int_to_text(ints):\n  try:\n    ints = ints.numpy()\n  except:\n    pass\n  return ''.join(idx2char[ints])\n\nprint(int_to_text(text_as_int[:13]))","execution_count":25,"outputs":[{"output_type":"stream","text":"First Citizen\n","name":"stdout"}]},{"metadata":{"id":"xBkXz9fjUQHW","trusted":true},"cell_type":"code","source":"seq_length = 100  # length of sequence for a training example\nexamples_per_epoch = len(text)//(seq_length+1)\n\n# Create training examples / targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)","execution_count":26,"outputs":[]},{"metadata":{"id":"Xi0xaPB_VOJl","trusted":true},"cell_type":"code","source":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)","execution_count":27,"outputs":[]},{"metadata":{"id":"03zKVHTvV0Km","trusted":true},"cell_type":"code","source":"def split_input_target(chunk):  # for the example: hello\n    input_text = chunk[:-1]  # hell\n    target_text = chunk[1:]  # ello\n    return input_text, target_text  # hell, ello\n\ndataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry","execution_count":28,"outputs":[]},{"metadata":{"id":"9p_y2YmgWbnc","trusted":true},"cell_type":"code","source":"for x, y in dataset.take(2):\n  print(\"\\n\\nEXAMPLE\\n\")\n  print(\"INPUT\")\n  print(int_to_text(x))\n  print(\"\\nOUTPUT\")\n  print(int_to_text(y))","execution_count":29,"outputs":[{"output_type":"stream","text":"\n\nEXAMPLE\n\nINPUT\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n\nOUTPUT\nirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou \n\n\nEXAMPLE\n\nINPUT\nare all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you \n\nOUTPUT\nre all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you k\n","name":"stdout"}]},{"metadata":{"id":"cRsKcjhXXuoD","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 64\nVOCAB_SIZE = len(vocab)  # vocab is number of unique characters\nEMBEDDING_DIM = 256\nRNN_UNITS = 1024\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndata = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)","execution_count":30,"outputs":[]},{"metadata":{"id":"5v_P2dEic4qt","trusted":true},"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.LSTM(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model\n\nmodel = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\nmodel.summary()","execution_count":31,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (64, None, 256)           16640     \n_________________________________________________________________\nlstm_1 (LSTM)                (64, None, 1024)          5246976   \n_________________________________________________________________\ndense_1 (Dense)              (64, None, 65)            66625     \n=================================================================\nTotal params: 5,330,241\nTrainable params: 5,330,241\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"id":"KdvEqlwc6_q0","trusted":true},"cell_type":"code","source":"for input_example_batch, target_example_batch in data.take(1):\n  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape","execution_count":32,"outputs":[{"output_type":"stream","text":"(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n","name":"stdout"}]},{"metadata":{"id":"RQS5KXwi7_NX","trusted":true},"cell_type":"code","source":"# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\nprint(len(example_batch_predictions))\nprint(example_batch_predictions)","execution_count":33,"outputs":[{"output_type":"stream","text":"64\ntf.Tensor(\n[[[ 2.97427177e-03 -8.60952213e-03  2.36647762e-03 ... -2.64788209e-03\n    3.23726516e-03  3.34957469e-04]\n  [-1.93846098e-03 -9.61866416e-03  1.76011748e-03 ... -5.29298745e-03\n    7.22827669e-03 -7.95011595e-03]\n  [ 1.80975755e-03 -2.32760678e-03  2.08490109e-03 ... -1.37031137e-03\n   -2.00028066e-03 -2.94426573e-03]\n  ...\n  [-1.43099520e-02 -5.84586058e-03  3.08889896e-04 ...  6.58532185e-03\n   -9.27647389e-03 -8.61037150e-03]\n  [-1.52042862e-02 -8.79837433e-04  5.01236878e-04 ...  5.90021070e-03\n   -1.37998043e-02 -7.35802390e-03]\n  [-1.13490261e-02  4.43530269e-03 -9.62882582e-03 ...  5.51317725e-03\n   -8.92911665e-03 -1.19916815e-02]]\n\n [[-3.46545759e-03 -1.49214233e-03 -2.37020105e-03 ...  7.40544288e-04\n   -2.60602334e-04 -9.75225121e-04]\n  [-8.29048548e-03 -3.61255556e-03 -4.59433440e-03 ... -6.15986064e-03\n   -1.19543099e-03 -1.84572442e-03]\n  [-5.06136380e-03 -6.37561595e-03 -4.81779687e-03 ... -7.35517731e-03\n   -8.57061520e-03  7.85052031e-03]\n  ...\n  [-1.12203853e-02 -8.85397568e-03 -1.72755984e-03 ... -3.15817306e-03\n   -8.16093199e-03 -5.40622883e-03]\n  [-5.31745329e-03 -1.10077364e-02  1.60583085e-03 ... -3.48441303e-03\n   -8.78998823e-03 -7.70983694e-04]\n  [-8.21707677e-03 -9.10066627e-03  1.64973550e-04 ... -5.87349338e-03\n   -2.25805724e-03 -9.96802188e-03]]\n\n [[-9.73959826e-03 -3.11099808e-03  1.02081383e-03 ...  1.20475155e-03\n   -2.77760811e-03 -4.42505814e-03]\n  [-5.25705889e-03 -8.01698864e-03  3.92488763e-03 ...  4.91685700e-04\n   -4.44796961e-03  9.02208732e-04]\n  [-8.35378468e-03 -5.97986206e-03 -1.59280258e-04 ...  1.69032067e-03\n   -4.32734564e-03 -3.21896048e-04]\n  ...\n  [-1.11211250e-02 -1.00153219e-02 -6.24752184e-03 ... -2.10613059e-03\n   -5.14016021e-03  4.58799768e-05]\n  [-1.18440948e-02 -9.54704173e-03 -9.18603875e-03 ... -1.16847618e-03\n   -4.72117402e-03 -1.32824504e-03]\n  [-7.01823365e-03 -5.66588109e-03 -8.14479217e-03 ... -9.66871623e-04\n   -5.01218904e-03  4.96673770e-03]]\n\n ...\n\n [[ 2.50811176e-03 -1.37575297e-03  1.40919699e-03 ...  9.88065964e-04\n    3.88145959e-03 -4.30514384e-03]\n  [ 3.63867707e-03 -3.76302050e-03  1.59575790e-03 ... -4.96393163e-03\n    6.66410662e-03 -1.02345005e-03]\n  [-6.73493929e-03 -6.48055458e-03  2.60913605e-03 ... -3.17289541e-03\n    2.57299514e-03 -4.98771062e-03]\n  ...\n  [-1.17933303e-02 -3.98696540e-03 -6.06460357e-03 ...  2.41472363e-03\n   -1.74457161e-03  4.73348331e-03]\n  [-5.78337675e-03  1.61894225e-03 -9.67262965e-03 ...  7.96573889e-03\n   -3.62987630e-05  2.66425195e-03]\n  [-6.75373105e-03 -3.30782705e-03 -9.94029269e-03 ...  2.19857367e-03\n    4.38151043e-03 -5.59468893e-03]]\n\n [[-3.32295848e-03 -1.50521356e-03  3.23493499e-04 ...  1.41979358e-03\n   -3.34370462e-03  1.93094555e-03]\n  [-6.67027489e-04 -5.95352566e-03  3.78619670e-03 ...  9.14870761e-05\n   -4.51586908e-03  4.77645360e-03]\n  [-1.14000505e-02 -5.51462080e-03  3.46060866e-03 ...  1.65791065e-03\n   -6.59774663e-03 -1.43736682e-03]\n  ...\n  [ 9.89839551e-04 -6.73374720e-03 -2.56981561e-03 ...  3.72513686e-03\n   -4.80146101e-03 -2.45422730e-03]\n  [-7.55131105e-03 -8.71413387e-03 -2.53044814e-03 ...  4.34733694e-03\n   -6.68886863e-03 -6.50605327e-03]\n  [-1.45815616e-03 -1.55169759e-02 -1.47412019e-03 ...  7.95039581e-04\n   -2.28008116e-03 -4.67093568e-03]]\n\n [[ 1.61573919e-03 -6.30869577e-03  1.98008539e-03 ... -4.93816566e-04\n   -1.77491677e-03 -4.56759566e-03]\n  [ 4.47871815e-03 -6.01950288e-03  1.67029467e-03 ...  3.20231123e-03\n    1.94271735e-03 -5.50546590e-03]\n  [ 1.27241679e-03 -6.19191863e-03  4.11467627e-04 ...  3.99674010e-03\n   -1.98753690e-03 -1.99980708e-03]\n  ...\n  [-1.52936100e-03 -1.17202885e-02 -3.88968317e-03 ...  6.39394391e-04\n   -8.58305302e-03 -5.98308397e-03]\n  [-1.11409025e-02 -9.83709656e-03 -2.60356301e-03 ...  2.24751700e-03\n   -1.14637259e-02 -9.95882601e-03]\n  [-1.19254719e-02 -7.42915971e-03 -5.52321970e-03 ...  3.09298979e-03\n   -1.08318301e-02 -8.97661597e-03]]], shape=(64, 100, 65), dtype=float32)\n","name":"stdout"}]},{"metadata":{"id":"sA1Zhop28V9n","trusted":true},"cell_type":"code","source":"# lets examine one prediction\npred = example_batch_predictions[0]\nprint(len(pred))\nprint(pred)\n# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step","execution_count":34,"outputs":[{"output_type":"stream","text":"100\ntf.Tensor(\n[[ 0.00297427 -0.00860952  0.00236648 ... -0.00264788  0.00323727\n   0.00033496]\n [-0.00193846 -0.00961866  0.00176012 ... -0.00529299  0.00722828\n  -0.00795012]\n [ 0.00180976 -0.00232761  0.0020849  ... -0.00137031 -0.00200028\n  -0.00294427]\n ...\n [-0.01430995 -0.00584586  0.00030889 ...  0.00658532 -0.00927647\n  -0.00861037]\n [-0.01520429 -0.00087984  0.00050124 ...  0.00590021 -0.0137998\n  -0.00735802]\n [-0.01134903  0.0044353  -0.00962883 ...  0.00551318 -0.00892912\n  -0.01199168]], shape=(100, 65), dtype=float32)\n","name":"stdout"}]},{"metadata":{"id":"UbIoe7Ei8q3q","trusted":true},"cell_type":"code","source":"# and finally well look at a prediction at the first timestep\ntime_pred = pred[0]\nprint(len(time_pred))\nprint(time_pred)\n# and of course its 65 values representing the probabillity of each character occuring next","execution_count":35,"outputs":[{"output_type":"stream","text":"65\ntf.Tensor(\n[ 2.9742718e-03 -8.6095221e-03  2.3664776e-03  4.4430867e-03\n  1.6158648e-03 -3.5532885e-03 -7.6198597e-03 -4.4533279e-04\n -1.2635207e-04 -3.3114601e-03 -1.4394526e-03  1.7470294e-03\n -2.6822928e-03 -3.5283687e-03 -4.9884384e-04 -3.6451791e-03\n  1.4439415e-03  1.0511301e-03  5.6317560e-03 -1.3140249e-03\n  6.0454831e-03  1.4361949e-04 -5.5921702e-03  6.0085859e-04\n  1.5024254e-03 -1.2327556e-03  3.7122001e-03 -1.9959910e-03\n  1.8040960e-03 -1.9908003e-03 -8.9391705e-04 -3.1445096e-03\n  2.1194227e-03 -3.1537567e-03  3.9227139e-03 -6.3412460e-03\n  5.6201182e-03  6.8359799e-04  1.7205307e-03  3.4345994e-03\n  7.5618341e-04 -5.4885950e-03  6.4768351e-04 -4.1522551e-05\n -3.3972594e-03  2.0607607e-05  1.5153288e-03  7.5561916e-03\n  3.3691032e-03  1.4330854e-03 -5.2745240e-03  4.8391067e-04\n -7.1525546e-03  3.2724985e-03 -8.9177385e-04 -4.2281032e-04\n -3.9885826e-03 -1.0015653e-03 -2.9996615e-03  2.1634933e-03\n -1.5927211e-03  7.0133910e-04 -2.6478821e-03  3.2372652e-03\n  3.3495747e-04], shape=(65,), dtype=float32)\n","name":"stdout"}]},{"metadata":{"id":"qlEYM1H995gR","trusted":true},"cell_type":"code","source":"# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\nsampled_indices = tf.random.categorical(pred, num_samples=1)\n\n# now we can reshape that array and convert all the integers to numbers to see the actual characters\nsampled_indices = np.reshape(sampled_indices, (1, -1))[0]\npredicted_chars = int_to_text(sampled_indices)\n\npredicted_chars  # and this is what the model predicted for training sequence 1","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"\"Lv?IpN:TuYz&XXHP FUDtfw;fGEStrQMdRciSyoCiNAQueTO3Ico3;QYW;SeL$'jpE:IOqAni\\n IOtv!oV:.I:ZvbRZ\\n:d&woSUH\""},"metadata":{}}]},{"metadata":{"id":"ZOw23fWq9D9O","trusted":true},"cell_type":"code","source":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","execution_count":37,"outputs":[]},{"metadata":{"id":"9g6o7zA_hAiS","trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss=loss)","execution_count":38,"outputs":[]},{"metadata":{"id":"v7aMushYjSpy","trusted":true},"cell_type":"code","source":"# Directory where the checkpoints will be saved\ncheckpoint_dir = './training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","execution_count":39,"outputs":[]},{"metadata":{"id":"R4PAgrwMjZ4_","trusted":true},"cell_type":"code","source":"history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])","execution_count":40,"outputs":[{"output_type":"stream","text":"Epoch 1/50\n172/172 [==============================] - 9s 54ms/step - loss: 2.7209\nEpoch 2/50\n172/172 [==============================] - 9s 54ms/step - loss: 2.0160\nEpoch 3/50\n172/172 [==============================] - 10s 55ms/step - loss: 1.7544\nEpoch 4/50\n172/172 [==============================] - 9s 53ms/step - loss: 1.6038\nEpoch 5/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.5103\nEpoch 6/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.4485\nEpoch 7/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.4027\nEpoch 8/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.3684\nEpoch 9/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.3375\nEpoch 10/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.3118\nEpoch 11/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.2873\nEpoch 12/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.2638\nEpoch 13/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.2408\nEpoch 14/50\n172/172 [==============================] - 10s 55ms/step - loss: 1.2200\nEpoch 15/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.1961\nEpoch 16/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.1725\nEpoch 17/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.1479\nEpoch 18/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.1235\nEpoch 19/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.0969\nEpoch 20/50\n172/172 [==============================] - 9s 55ms/step - loss: 1.0689\nEpoch 21/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.0412\nEpoch 22/50\n172/172 [==============================] - 9s 54ms/step - loss: 1.0101\nEpoch 23/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.9814\nEpoch 24/50\n172/172 [==============================] - 9s 53ms/step - loss: 0.9504\nEpoch 25/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.9187\nEpoch 26/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.8883\nEpoch 27/50\n172/172 [==============================] - 9s 53ms/step - loss: 0.8574\nEpoch 28/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.8281\nEpoch 29/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.7996\nEpoch 30/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.7706\nEpoch 31/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.7460\nEpoch 32/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.7194\nEpoch 33/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.6968\nEpoch 34/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.6738\nEpoch 35/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.6535\nEpoch 36/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.6344\nEpoch 37/50\n172/172 [==============================] - 9s 55ms/step - loss: 0.6169\nEpoch 38/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.6000\nEpoch 39/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.5858\nEpoch 40/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.5700\nEpoch 41/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.5576\nEpoch 42/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.5435\nEpoch 43/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.5321\nEpoch 44/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.5217\nEpoch 45/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.5106\nEpoch 46/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.5018\nEpoch 47/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.4931\nEpoch 48/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.4872\nEpoch 49/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.4794\nEpoch 50/50\n172/172 [==============================] - 9s 54ms/step - loss: 0.4728\n","name":"stdout"}]},{"metadata":{"id":"TPSto3uimSKp","trusted":true},"cell_type":"code","source":"model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)","execution_count":41,"outputs":[]},{"metadata":{"id":"PZIEZWE4mNKl","trusted":true},"cell_type":"code","source":"model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))","execution_count":42,"outputs":[]},{"metadata":{"id":"oPSALdQXA3l3","trusted":true},"cell_type":"code","source":"def generate_text(model, start_string):\n  # Evaluation step (generating text using the learned model)\n\n  # Number of characters to generate\n  num_generate = 800\n\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  temperature = 1.0\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n    \n      predictions = tf.squeeze(predictions, 0)\n\n      # using a categorical distribution to predict the character returned by the model\n      predictions = predictions / temperature\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n      # We pass the predicted character as the next input to the model\n      # along with the previous hidden state\n      input_eval = tf.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n\n  return (start_string + ''.join(text_generated))","execution_count":null,"outputs":[]},{"metadata":{"id":"cAJqhD9AA5mF","trusted":true},"cell_type":"code","source":"inp = input(\"Type a starting string: \")\nprint(generate_text(model, inp))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}